{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5e2803a-954b-4226-a52f-89471427c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re \n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "\n",
    "def preprocess_aicity_conversations_for_script(conversations_list):\n",
    "    \"\"\"\n",
    "    Modifies conversations:\n",
    "    1. Prepends '<image>\\n' to the first human turn's value.\n",
    "    2. Replaces '<mask>' with '<mask> <depth>' in all human turns' values.\n",
    "    \"\"\"\n",
    "    processed_conversations = []\n",
    "    is_first_human_turn_overall = True  # To ensure <image>\\n is only added once at the very beginning\n",
    "    for i, turn in enumerate(conversations_list):\n",
    "        new_turn = turn.copy()\n",
    "        if new_turn.get(\"from\") == \"human\":\n",
    "            current_value = new_turn[\"value\"]\n",
    "            # Add <image>\\n to the start of the first human utterance in the conversation\n",
    "            if is_first_human_turn_overall:\n",
    "                if not current_value.strip().startswith(DEFAULT_IMAGE_TOKEN):  # DEFAULT_IMAGE_TOKEN is \"<image>\"\n",
    "                    current_value = DEFAULT_IMAGE_TOKEN + \"\\n\" + current_value\n",
    "                is_first_human_turn_overall = False\n",
    "\n",
    "            # Replace <mask> with <mask> <depth>\n",
    "            current_value = re.sub(r\"<mask>\", \"<mask> <depth>\", current_value)\n",
    "            new_turn[\"value\"] = current_value\n",
    "        processed_conversations.append(new_turn)\n",
    "    return processed_conversations\n",
    "\n",
    "\n",
    "def convert_aicity_to_spatialrgpt_format(aicity_json_path, output_json_path):\n",
    "\n",
    "    # load anotation file (json)\n",
    "    print(f\"Loading AI City data from: {aicity_json_path}\")\n",
    "    with open(aicity_json_path, \"r\") as f:\n",
    "        aicity_data = json.load(f)\n",
    "\n",
    "    print(f\"Found {len(aicity_data)} samples. Converting...\")\n",
    "    skipped_samples = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # open output file to write\n",
    "    with open(output_json_path, \"w\") as outfile:\n",
    "        # loop each sample\n",
    "        for sample_idx, sample in tqdm(enumerate(aicity_data), desc=\"Converting ...\"):\n",
    "            try:\n",
    "                # take image\n",
    "                image_filename_ext = sample.get(\"image\")\n",
    "                if not image_filename_ext or not isinstance(image_filename_ext, str):\n",
    "                    print(\n",
    "                        f\"Warning: Skipping sample ID {sample.get('id')} (index {sample_idx}) due to missing/invalid 'image' field.\"\n",
    "                    )\n",
    "                    skipped_samples += 1\n",
    "                    continue\n",
    "\n",
    "                # take basename of image path: 000001.png -> 000001\n",
    "                filename_base, _ = os.path.splitext(os.path.basename(image_filename_ext))\n",
    "\n",
    "                # take conversation\n",
    "                if (\n",
    "                    not sample.get(\"conversations\")\n",
    "                    or not isinstance(sample[\"conversations\"], list)\n",
    "                    or not sample[\"conversations\"]\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"Warning: Skipping sample ID {sample.get('id')} (index {sample_idx}) due to missing/invalid 'conversations'.\"\n",
    "                    )\n",
    "                    skipped_samples += 1\n",
    "                    continue\n",
    "\n",
    "                modified_conversations = preprocess_aicity_conversations_for_script(sample[\"conversations\"])\n",
    "\n",
    "                # take rle msk\n",
    "                rle_data = sample.get(\"rle\", [])  # Default to empty list if missing\n",
    "                if not isinstance(rle_data, list):\n",
    "                    print(\n",
    "                        f\"Warning: RLE data for sample ID {sample.get('id')} (index {sample_idx}) is not a list. Using empty list.\"\n",
    "                    )\n",
    "                    rle_data = []\n",
    "\n",
    "                # merge and write to output file\n",
    "                formatted_sample = {\n",
    "                    \"id\": sample[\"id\"],\n",
    "                    \"image_base_filename\": filename_base,  # For AICityLazySpatialDataset\n",
    "                    \"conversations\": modified_conversations,\n",
    "                    \"rle\": rle_data,\n",
    "                    \"category\": sample.get(\"category\", \"unknown\"),  # Keep for reference\n",
    "                    \"region_labels\": sample.get(\"region_labels\")\n",
    "                }\n",
    "                outfile.write(json.dumps(formatted_sample) + \"\\n\")\n",
    "                total_samples += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample ID {sample.get('id')} (index {sample_idx}): {e}\")\n",
    "                skipped_samples += 1\n",
    "\n",
    "    print(f\"Conversion complete for '{aicity_json_path}'. Output saved to: '{output_json_path}'\\nTotal sample: {total_samples}\")\n",
    "    if skipped_samples > 0:\n",
    "        print(f\"Skipped {skipped_samples} samples due to issues.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "\n",
    "#     base_raw_data_dir = \"datasets/PhysicalAI-Spatial-Intelligence-Warehouse\" \n",
    "#     # original_train_json = os.path.join(base_raw_data_dir, \"train.json\")\n",
    "#     # original_train_sample_json = os.path.join(base_raw_data_dir, \"train_sample/train_sample.json\")\n",
    "#     original_train_json = os.path.join(base_raw_data_dir, \"train.json\")\n",
    "#     original_val_json = os.path.join(base_raw_data_dir, \"val.json\")\n",
    "#     original_test_json = os.path.join(base_raw_data_dir, \"test.json\")\n",
    "\n",
    "#     # Paths for processed output data\n",
    "#     processed_data_dir = \"datasets/PhysicalAI-Spatial-Intelligence-Warehouse/formatted_dataset\"  # Script will create this\n",
    "#     os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "#     # processed_train_sample_jsonl = os.path.join(processed_data_dir, \"train_aicity_srgpt.jsonl\")\n",
    "#     processed_train_jsonl = os.path.join(processed_data_dir, \"train_aicity_srgpt.jsonl\")\n",
    "#     processed_val_jsonl = os.path.join(processed_data_dir, \"val_aicity_srgpt.jsonl\")\n",
    "#     processed_test_jsonl = os.path.join(processed_data_dir, \"test_aicity_srgpt.jsonl\")\n",
    "\n",
    "#     print(\"Starting AI City Dataset Conversion for SpatialRGPT fine-tuning...\")\n",
    "\n",
    "#     if os.path.exists(original_train_json):\n",
    "#         convert_aicity_to_spatialrgpt_format(original_train_json, processed_train_jsonl)\n",
    "#     else:\n",
    "#         print(f\"ERROR: AI City train.json not found at {original_train_json}. Please check the path.\")\n",
    "\n",
    "#     if os.path.exists(original_val_json):\n",
    "#         convert_aicity_to_spatialrgpt_format(original_val_json, processed_val_jsonl)\n",
    "#     else:\n",
    "#         print(f\"ERROR: AI City val.json not found at {original_val_json}. Please check the path.\")\n",
    "\n",
    "#     if os.path.exists(original_test_json):\n",
    "#         convert_aicity_to_spatialrgpt_format(original_test_json, processed_test_jsonl)\n",
    "#     else:\n",
    "#         print(f\"ERROR: AI City val.json not found at {original_test_json}. Please check the path.\")\n",
    "\n",
    "#     print(\"Dataset conversion script finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910692d-acc9-43ac-8eb3-0e9784d6c098",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset for region classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af370930-1fd2-433e-9339-2a2a3e2bb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 499083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '26ba60afef390047b84ee839e8f7cee3',\n",
       " 'image': '070760.png',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nTell me the distance between the pallet <mask> and the pallet <mask>.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The distance of the pallet [Region 0] from the pallet [Region 1] is 2.32 meters.'}],\n",
       " 'rle': [{'size': [1080, 1920],\n",
       "   'counts': 'k\\\\R?<[Q1010O103L3M3N3L3N3L3N2M4L3N2M4M2XQOlNgl0X1WSOgNhl0\\\\1WSOcNil0_1VSO`Nil0d1USO[Nkl0g1TSOXNkl0k1hROoM]O<dm0h1lROnM_O:cm0k1kROlMC9`m0o1iROjMG7_m0Q2hROiMI6]m0T2gROhML4\\\\m0W2eROfMO3Zm0Z2dROeM21Xm0]2cROdM5OWm0_2bROcM7NUm0T3kROlLTm0U3lROkLRm0V3oROjLPm0W3PSOiLnl0Y3RSOgLll0[3SSOfLll0[3TSOeLjl0]3VSOcLil0]3XSOcLfl0_3ZSOaLdl0a3\\\\SO^Ldl0c3\\\\SO]Lbl0e3^SO[Lal0f3_SOZL_l0i3`SOWL_l0j3aSOVL]l0l3cSOTL[l0n3eSORLZl0o3fSOQLXl0Q4hSOoKWl0R4iSOnKUl0T4kSOkKTl0W4lSOhKTl0Y4lSOeKTl0]4kSObKVl0_4iSOaKVl0a4hSO_KYl0a4fSO_KZl0c4eSO]KZl0P51O010O00010O0011N2O1N3M2O1N2O1N2N2O1N2O1N2O1N2N100O100O1O100O100O010O02N2OO0001K5O1O1N2YJVTO_5kk0aJVTO]5kk0bJWTO]5ik0cJXTO[5ik0dJYTOZ5hk0eJZTOZ5fk0fJZTOY5Ul0M101N2N2O0O2N2O1N2N2O0OIZK_SOd4bl0^K]SOa4cl0`K]SO^4dl0cK\\\\SO[4el0eK\\\\SOZ4dl0gK[SOX4fl0iKZSOU4gl0lKYSOT4fl0mKZSOR4fl0oKZSOP4fl0PL[SOP4dl0QL[SOP4dl0QL\\\\SOn3dl0SL\\\\SOl3dl0UL\\\\SOi3el0XL[SOf3fl0ZL[SOe3el0\\\\LZSOc3gl0^LYSO`3hl0aLXSO^3hl0cLXSO[3em0N0O1J7N2O1N2L4OKcLoQO\\\\3Rn0eLnQOY3Sn0hLmQOW3Sn0jLmQOT3Tn0lLmQOR3Tn0oLlQOP3Tn0QMkQOn2Vn0SMjQOk2Wn0VMiQOi2Wn0XMiQOf2Xn0ZMjQOc2Wn0^MlQO^2Tn0cMnQOY2Sn0hMPROS2Qn0nMQROo1om0RNTROi1mm0WNVROe1km0\\\\NXRO`1hm0aN[ROZ1fm0gN\\\\ROV1Yo0I2N5L3L3NO2O1NSOaoNd0]P1]OdoNb0\\\\P1]OfoNb0iP1N2O1N2N101N2M3K5N2N2O_miY1'},\n",
       "  {'size': [1080, 1920],\n",
       "   'counts': 'ojgg03dQ16J5K6K5coNZOc0:XOLPm0e0PSOWOg07[OLml0l0kROSOEMQ18CLll0Q1fROUOh0NHKil0X1aROQOl0MKIgl0l1]SOZNNIel0m1\\\\SOZN1Hbl0P2[SOYN4F`l0S2[SOVN7F^l0U2YSOUN;E[l0X2XSOSNjm0o1TRORNlm0n1TROQNlm0Q2RROoMnm0S2PROmMQn0T2nQOlMQn0V2mQOjMSn0X2kQOhMVn0Y2hQOhMWn0Y2iQOfMWn0\\\\2gQOdMZn0]2dQOcM\\\\n0_2cQOaM\\\\n0m21O00010O0001O010O1O2O1N2N2O1N1O2O1N2N2O1N2N101N2N2O1N2N101N2N2O1N2N2O0O2N2N2O1N2N101N2N2O1N2N2O0O2N2O1N2N2O0O2N2O1N2N2O10O10O1000000O01O1O1O001N2N2O0O2N2N2N100O1K41O01O00010O000010O00000100O1O2N1O2O1N2O010O1N2O0O2N2N2O0O2N2N101N2N2K4O2N2N2O0O2N2N2N101N2N2N101N2N2N101N4MOVMgQOX2^o0^O<Ca0@8G2N2N2N101N2BnnN3mcjQ1'}],\n",
       " 'category': 'distance',\n",
       " 'normalized_answer': 2.32,\n",
       " 'freeform_answer': 'The distance of the pallet [Region 0] from the pallet [Region 1] is 2.32 meters.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_raw_data_dir = \"datasets/PhysicalAI-Spatial-Intelligence-Warehouse\" \n",
    "# original_train_json = os.path.join(base_raw_data_dir, \"train.json\")\n",
    "# original_train_sample_json = os.path.join(base_raw_data_dir, \"train_sample/train_sample.json\")\n",
    "original_train_json = os.path.join(base_raw_data_dir, \"train.json\")\n",
    "original_val_json = os.path.join(base_raw_data_dir, \"val.json\")\n",
    "original_test_json = os.path.join(base_raw_data_dir, \"test.json\")\n",
    "\n",
    "# load train data\n",
    "with open(original_train_json, \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "print(f\"Length of train data: {len(train_data)}\")\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ee6558-215d-4e8c-9637-34859fed7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Category Keywords and Mapping ---\n",
    "# Maps keywords found in text to a canonical label. Handles plurals and synonyms.\n",
    "KEYWORD_TO_LABEL_MAP = {\n",
    "    \"<mask> among\": \"buffer\",\n",
    "    \"among buffer regions\": \"buffer\",\n",
    "    \"among the pallet\": \"pallet\",\n",
    "    \"among the pallets\": \"pallet\",\n",
    "    \"and pallet masks\": \"pallet\",\n",
    "    \"and pallets\": \"pallet\",\n",
    "    \"and shelves\": \"shelf\",\n",
    "    \"and the pallet\": \"pallet\",\n",
    "    \"and the pallets\": \"pallet\",\n",
    "    \"and the shelf\": \"shelf\",\n",
    "    \"and the shelves\": \"shelf\",\n",
    "    \"are the pallet\": \"pallet\",\n",
    "    \"available pallets in\": \"pallet\",\n",
    "    \"available transporter in\": \"transporter\",\n",
    "    \"between the pallet\": \"pallet\",\n",
    "    \"buffer area among\": \"buffer\",\n",
    "    \"buffer area in\": \"buffer\",\n",
    "    \"buffer area within\": \"buffer\",\n",
    "    \"buffer region among\": \"buffer\",\n",
    "    \"buffer region from\": \"buffer\",\n",
    "    \"buffer region in\": \"buffer\",\n",
    "    \"buffer region within\": \"buffer\",\n",
    "    \"buffer zone in\": \"buffer\",\n",
    "    \"buffer zones in\": \"buffer\",\n",
    "    \"considering the pallets\": \"pallet\",\n",
    "    \"considering the transporters\": \"transporter\",\n",
    "    \"distance between transporter\": \"transporter\",\n",
    "    \"distance from transporter\": \"transporter\",\n",
    "    \"distance of transporter\": \"transporter\",\n",
    "    \"does the pallet\": \"pallet\",\n",
    "    \"empty transporter in\": \"transporter\",\n",
    "    \"far is transporter\": \"transporter\",\n",
    "    \"for the transporter\": \"transporter\",\n",
    "    \"from the pallet\": \"pallet\",\n",
    "    \"from the shelf\": \"shelf\",\n",
    "    \"given buffer masks\": \"buffer\",\n",
    "    \"given buffer zones\": \"buffer\",\n",
    "    \"given the pallets\": \"pallet\",\n",
    "    \"given the transporters\": \"transporter\",\n",
    "    \"idle transporter in\": \"transporter\",\n",
    "    \"if the pallet\": \"pallet\",\n",
    "    \"is closest to\": \"shelf\",\n",
    "    \"is the pallet\": \"pallet\",\n",
    "    \"locations of pallets\": \"pallet\",\n",
    "    \"most convenient for\": \"transporter\",\n",
    "    \"of the pallet\": \"pallet\",\n",
    "    \"pallet positions in\": \"pallet\",\n",
    "    \"position of pallets\": \"pallet\",\n",
    "    \"provided buffer masks\": \"buffer\",\n",
    "    \"the available transporters\": \"transporter\",\n",
    "    \"the buffer masks\": \"buffer\",\n",
    "    \"the buffer region\": \"buffer\",\n",
    "    \"the buffer regions\": \"buffer\",\n",
    "    \"the buffer zones\": \"buffer\",\n",
    "    \"the current transporters\": \"transporter\",\n",
    "    \"the left among\": \"shelf\",\n",
    "    \"the nearest to\": \"shelf\",\n",
    "    \"the pallet\": \"pallet\",\n",
    "    \"the pallets in\": \"pallet\",\n",
    "    \"the placement of\": \"pallet\",\n",
    "    \"the right among\": \"shelf\",\n",
    "    \"the transporter at\": \"transporter\",\n",
    "    \"the transporter in\": \"transporter\",\n",
    "    \"there between transporter\": \"transporter\",\n",
    "    \"to the pallet\": \"pallet\",\n",
    "    \"to the shelf\": \"shelf\",\n",
    "    \"which pallet from\": \"pallet\",\n",
    "    \"which pallet in\": \"pallet\",\n",
    "    \"which pallet within\": \"pallet\",\n",
    "}\n",
    "category_to_id = {\n",
    "        \"pallet\": 0,\n",
    "        \"buffer\": 1,\n",
    "        \"shelf\": 2, \n",
    "        \"transporter\": 3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee3cdcc-0ff5-4b9d-91cd-7f10a56d8cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add region labels: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499083/499083 [00:05<00:00, 91308.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '26ba60afef390047b84ee839e8f7cee3',\n",
       " 'image': '070760.png',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nTell me the distance between the pallet <mask> and the pallet <mask>.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The distance of the pallet [Region 0] from the pallet [Region 1] is 2.32 meters.'}],\n",
       " 'rle': [{'size': [1080, 1920],\n",
       "   'counts': 'k\\\\R?<[Q1010O103L3M3N3L3N3L3N2M4L3N2M4M2XQOlNgl0X1WSOgNhl0\\\\1WSOcNil0_1VSO`Nil0d1USO[Nkl0g1TSOXNkl0k1hROoM]O<dm0h1lROnM_O:cm0k1kROlMC9`m0o1iROjMG7_m0Q2hROiMI6]m0T2gROhML4\\\\m0W2eROfMO3Zm0Z2dROeM21Xm0]2cROdM5OWm0_2bROcM7NUm0T3kROlLTm0U3lROkLRm0V3oROjLPm0W3PSOiLnl0Y3RSOgLll0[3SSOfLll0[3TSOeLjl0]3VSOcLil0]3XSOcLfl0_3ZSOaLdl0a3\\\\SO^Ldl0c3\\\\SO]Lbl0e3^SO[Lal0f3_SOZL_l0i3`SOWL_l0j3aSOVL]l0l3cSOTL[l0n3eSORLZl0o3fSOQLXl0Q4hSOoKWl0R4iSOnKUl0T4kSOkKTl0W4lSOhKTl0Y4lSOeKTl0]4kSObKVl0_4iSOaKVl0a4hSO_KYl0a4fSO_KZl0c4eSO]KZl0P51O010O00010O0011N2O1N3M2O1N2O1N2N2O1N2O1N2O1N2N100O100O1O100O100O010O02N2OO0001K5O1O1N2YJVTO_5kk0aJVTO]5kk0bJWTO]5ik0cJXTO[5ik0dJYTOZ5hk0eJZTOZ5fk0fJZTOY5Ul0M101N2N2O0O2N2O1N2N2O0OIZK_SOd4bl0^K]SOa4cl0`K]SO^4dl0cK\\\\SO[4el0eK\\\\SOZ4dl0gK[SOX4fl0iKZSOU4gl0lKYSOT4fl0mKZSOR4fl0oKZSOP4fl0PL[SOP4dl0QL[SOP4dl0QL\\\\SOn3dl0SL\\\\SOl3dl0UL\\\\SOi3el0XL[SOf3fl0ZL[SOe3el0\\\\LZSOc3gl0^LYSO`3hl0aLXSO^3hl0cLXSO[3em0N0O1J7N2O1N2L4OKcLoQO\\\\3Rn0eLnQOY3Sn0hLmQOW3Sn0jLmQOT3Tn0lLmQOR3Tn0oLlQOP3Tn0QMkQOn2Vn0SMjQOk2Wn0VMiQOi2Wn0XMiQOf2Xn0ZMjQOc2Wn0^MlQO^2Tn0cMnQOY2Sn0hMPROS2Qn0nMQROo1om0RNTROi1mm0WNVROe1km0\\\\NXRO`1hm0aN[ROZ1fm0gN\\\\ROV1Yo0I2N5L3L3NO2O1NSOaoNd0]P1]OdoNb0\\\\P1]OfoNb0iP1N2O1N2N101N2M3K5N2N2O_miY1'},\n",
       "  {'size': [1080, 1920],\n",
       "   'counts': 'ojgg03dQ16J5K6K5coNZOc0:XOLPm0e0PSOWOg07[OLml0l0kROSOEMQ18CLll0Q1fROUOh0NHKil0X1aROQOl0MKIgl0l1]SOZNNIel0m1\\\\SOZN1Hbl0P2[SOYN4F`l0S2[SOVN7F^l0U2YSOUN;E[l0X2XSOSNjm0o1TRORNlm0n1TROQNlm0Q2RROoMnm0S2PROmMQn0T2nQOlMQn0V2mQOjMSn0X2kQOhMVn0Y2hQOhMWn0Y2iQOfMWn0\\\\2gQOdMZn0]2dQOcM\\\\n0_2cQOaM\\\\n0m21O00010O0001O010O1O2O1N2N2O1N1O2O1N2N2O1N2N101N2N2O1N2N101N2N2O1N2N2O0O2N2N2O1N2N101N2N2O1N2N2O0O2N2O1N2N2O0O2N2O1N2N2O10O10O1000000O01O1O1O001N2N2O0O2N2N2N100O1K41O01O00010O000010O00000100O1O2N1O2O1N2O010O1N2O0O2N2N2O0O2N2N101N2N2K4O2N2N2O0O2N2N2N101N2N2N101N2N2N101N4MOVMgQOX2^o0^O<Ca0@8G2N2N2N101N2BnnN3mcjQ1'}],\n",
       " 'category': 'distance',\n",
       " 'normalized_answer': 2.32,\n",
       " 'freeform_answer': 'The distance of the pallet [Region 0] from the pallet [Region 1] is 2.32 meters.',\n",
       " 'region_labels': ['pallet', 'pallet']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def assign_region_labels(question: str, keyword_to_label_map: dict):\n",
    "    question_lower = question.lower()\n",
    "\n",
    "    # Find all <mask> groups\n",
    "    mask_pattern = re.compile(r'((?:<mask>\\s*)+)')\n",
    "    mask_spans = list(mask_pattern.finditer(question_lower))\n",
    "\n",
    "    sorted_keywords = sorted(keyword_to_label_map.keys(), key=len, reverse=True)\n",
    "    labels = []\n",
    "\n",
    "    for match in mask_spans:\n",
    "        start_idx = match.start()\n",
    "\n",
    "        # Slice the text immediately before this <mask> group (e.g., last 50 chars before)\n",
    "        context_before_mask = question_lower[max(0, start_idx - 30):start_idx].rstrip()\n",
    "\n",
    "        label_found = None\n",
    "        for keyword in sorted_keywords:\n",
    "            if context_before_mask.endswith(keyword):\n",
    "                label_found = keyword_to_label_map[keyword]\n",
    "                break\n",
    "\n",
    "        mask_count = match.group(0).count('<mask>')\n",
    "        if label_found is None:\n",
    "            raise ValueError(f\"No matching keyword found for mask group: {match.group(0)}\")\n",
    "\n",
    "        labels.extend([label_found] * mask_count)\n",
    "\n",
    "    return labels\n",
    "def add_region_label(train_data: dict):\n",
    "    train_data_added_region_labels = []\n",
    "    for sample in tqdm(train_data, desc=\"Add region labels\"):\n",
    "        new_sample = sample\n",
    "        # take question\n",
    "        for turn in sample['conversations']:\n",
    "            if turn.get('from') == 'human':\n",
    "                question = turn.get('value')\n",
    "\n",
    "                # get label of region\n",
    "                region_labels = assign_region_labels(question, KEYWORD_TO_LABEL_MAP)\n",
    "                if len(region_labels) != len(sample['rle']):\n",
    "                    print(\"Warning: number of region and labels is not match!!!\")\n",
    "                    region_labels = [None]\n",
    "                # add to new train data\n",
    "                new_sample['region_labels'] = region_labels\n",
    "                train_data_added_region_labels.append(new_sample)\n",
    "\n",
    "    return train_data_added_region_labels\n",
    "    \n",
    "train_data_added_region_labels = add_region_label(train_data)\n",
    "train_data_added_region_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc3a3d3-bf8a-43d5-8b09-cd34e081ee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['pallet', 'buffer', 'shelf', 'transporter']\n",
      "All region labels add correctly :))\n"
     ]
    }
   ],
   "source": [
    "# check correctness of region label\n",
    "categories = list(category_to_id.keys())\n",
    "print(f\"Categories: {categories}\")\n",
    "\n",
    "count_correct = 0\n",
    "for sample in train_data_added_region_labels:\n",
    "    region_labels = sample.get('region_labels')\n",
    "    if len(region_labels) == len(sample.get('rle')) and all([c in categories for c in region_labels]):\n",
    "        count_correct += 1\n",
    "if count_correct == len(train_data_added_region_labels) and count_correct == len(train_data):\n",
    "    print(f\"All region labels add correctly :))\")\n",
    "else: \n",
    "    print(f'Few region labels not correct added :((')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8766460-7338-4561-aaec-82c2948c5f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save new data to: datasets/PhysicalAI-Spatial-Intelligence-Warehouse/data_for_region_classification/train_data_added_region_label.json\n"
     ]
    }
   ],
   "source": [
    "# Write new train data\n",
    "save_path = \"datasets/PhysicalAI-Spatial-Intelligence-Warehouse/data_for_region_classification/train_data_added_region_label.json\"\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(train_data_added_region_labels, f, indent=2)\n",
    "print(f\"Save new data to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e61b977-b257-425f-8bf1-5867403e34de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI City data from: datasets/PhysicalAI-Spatial-Intelligence-Warehouse/data_for_region_classification/train_data_added_region_label.json\n",
      "Found 499083 samples. Converting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting ...: 499083it [00:14, 34102.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete for 'datasets/PhysicalAI-Spatial-Intelligence-Warehouse/data_for_region_classification/train_data_added_region_label.json'. Output saved to: 'datasets/PhysicalAI-Spatial-Intelligence-Warehouse/data_for_region_classification/train_data_added_region_label_formatted.jsonl'\n",
      "Total sample: 499083\n"
     ]
    }
   ],
   "source": [
    "aicity_json_path = \"datasets/PhysicalAI-Spatial-Intelligence-Warehouse/data_for_region_classification/train_data_added_region_label.json\"\n",
    "output_json_path = \"datasets/PhysicalAI-Spatial-Intelligence-Warehouse/data_for_region_classification/train_data_added_region_label_formatted.jsonl\"\n",
    "convert_aicity_to_spatialrgpt_format(aicity_json_path, output_json_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
