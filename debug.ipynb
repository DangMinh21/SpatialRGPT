{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de25f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dangminh/Desktop/SpatialRGPT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c7529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sys.path to include: /Users/dangminh/Desktop/SpatialRGPT\n",
      "Current working directory: /Users/dangminh/Desktop/SpatialRGPT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/intern/flash_attention.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# v1\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_attn_unpadded_qkvpacked_func\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# v2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, AutoTokenizer, AutoImageProcessor\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataArguments \u001b[38;5;66;03m# Still need this for structuring data_args\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazySpatialWarehouseDataset\n\u001b[1;32m     39\u001b[0m  \u001b[38;5;66;03m# Your custom class\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX,\n\u001b[1;32m     42\u001b[0m     DEFAULT_MASK_TOKEN, DEFAULT_DEPTH_TOKEN,\n\u001b[1;32m     43\u001b[0m     DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, DEFAULT_IMAGE_PATCH_TOKEN\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/data/dataset.py:59\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# from llava.eval.mm_utils.data_utils import CAT_SHORT2LONG, construct_prompt, load_yaml, process_single_sample\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     52\u001b[0m     is_gemma_tokenizer,\n\u001b[1;32m     53\u001b[0m     opencv_extract_frames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     tokenizer_image_token,\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataArguments, TrainingArguments\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     extract_local_from_list,\n\u001b[1;32m     63\u001b[0m     extract_local_input_ids,\n\u001b[1;32m     64\u001b[0m     extract_local_position_ids,\n\u001b[1;32m     65\u001b[0m     get_pg_manager,\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaLlamaConfig, LlavaLlamaModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_mistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaMistralConfig, LlavaMistralForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_mixtral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaMixtralConfig, LlavaMixtralForCausalLM\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/language_model/llava_llama.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalLMOutputWithPast\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_llava\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaConfig\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_arch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaMetaForCausalLM, LlavaMetaModel\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vision_tower\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_projector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_mm_projector\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/llava_arch.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_llava\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaConfig\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_llm_and_tokenizer\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vision_tower\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_projector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_mm_projector\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregion_extractor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_region_extractor\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/builder.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, PretrainedConfig, PreTrainedModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPVisionTower, CLIPVisionTowerS2\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintern_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternVisionTower\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mradio_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RADIOVisionTower\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msiglip_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SiglipVisionTower, SiglipVisionTowerS2\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/intern_encoder.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_intern_vit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternVisionConfig\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_intern_vit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternVisionModel\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisionTower\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_transform\u001b[39m(input_size):\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/intern/modeling_intern_vit.py:20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_intern_vit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternVisionConfig\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlashAttention\n\u001b[1;32m     22\u001b[0m has_flash_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/intern/flash_attention.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_attn_unpadded_qkvpacked_func\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# v2\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_attn_varlen_qkvpacked_func \u001b[38;5;28;01mas\u001b[39;00m flash_attn_unpadded_qkvpacked_func\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert_padding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_input, unpad_input\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFlashAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # For optional visualization\n",
    "import sys\n",
    "\n",
    "# --- Configuration: Update these paths for your local machine ---\n",
    "SPATIAL_RGPT_REPO_PATH = \"./SpatialRGPT\"  # Path to your cloned SpatialRGPT repository\n",
    "PROCESSED_JSONL_PATH = \"./test_data/aicity_srgpt_sample.jsonl\" # Path to your small test JSONL\n",
    "RGB_IMAGE_BASE_DIR = \"./test_data/aicity_sample_data/train/images/\"\n",
    "DEPTH_IMAGE_BASE_DIR = \"./test_data/aicity_sample_data/train/depths/\"\n",
    "# Model path for tokenizer & image_processor (can be a local path if you downloaded, or HF ID)\n",
    "BASE_MODEL_PATH_FOR_CONFIG = \"a8cheng/SpatialRGPT-VILA1.5-8B\" \n",
    "\n",
    "# Add SpatialRGPT to Python path for imports\n",
    "sys.path.insert(0, os.path.abspath(SPATIAL_RGPT_REPO_PATH))\n",
    "\n",
    "try:\n",
    "    from llava.train.args import DataArguments\n",
    "    from llava.model.builder import load_pretrained_model\n",
    "    from llava.data.aicity_dataset import AICityLazySpatialDataset # Your custom class\n",
    "    from llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "    from llava.mm_utils import get_model_name_from_path\n",
    "    from llava.train.transformer_normalize_monkey_patch import patch_normalize_preprocess\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from LLaVA/SpatialRGPT. Ensure SPATIAL_RGPT_REPO_PATH is correct and it's on sys.path: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def create_dummy_data_if_not_exists():\n",
    "    \"\"\"Creates dummy data and folder structure if they don't exist for testing.\"\"\"\n",
    "    os.makedirs(os.path.dirname(PROCESSED_JSONL_PATH), exist_ok=True)\n",
    "    os.makedirs(RGB_IMAGE_BASE_DIR, exist_ok=True)\n",
    "    os.makedirs(DEPTH_IMAGE_BASE_DIR, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(PROCESSED_JSONL_PATH):\n",
    "        print(f\"Creating dummy JSONL: {PROCESSED_JSONL_PATH}\")\n",
    "        dummy_data = [\n",
    "            {\n",
    "                \"id\": \"dummy_sample_0\", \"image_base_filename\": \"dummy_rgb_0\",\n",
    "                \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nTest question with <mask> <depth>\"}, {\"from\": \"gpt\", \"value\": \"Test answer.\"}],\n",
    "                \"rle\": [{\"size\": [100,100], \"counts\": \"RLE0\"}] # Simplified RLE for structure\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"dummy_sample_1\", \"image_base_filename\": \"dummy_rgb_1\",\n",
    "                \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nAnother <mask> <depth> and <mask> <depth>\"}, {\"from\": \"gpt\", \"value\": \"Another answer.\"}],\n",
    "                \"rle\": [{\"size\": [100,100], \"counts\": \"RLE1\"}, {\"size\": [100,100], \"counts\": \"RLE2\"}]\n",
    "            }\n",
    "        ]\n",
    "        with open(PROCESSED_JSONL_PATH, 'w') as f:\n",
    "            for entry in dummy_data:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "    for i in range(2): # Create 2 dummy images\n",
    "        try:\n",
    "            img_path = os.path.join(RGB_IMAGE_BASE_DIR, f\"dummy_rgb_{i}.png\")\n",
    "            depth_path = os.path.join(DEPTH_IMAGE_BASE_DIR, f\"dummy_rgb_{i}_depth.png\")\n",
    "            if not os.path.exists(img_path):\n",
    "                Image.new('RGB', (100, 100), color='blue').save(img_path)\n",
    "                print(f\"Created dummy RGB image: {img_path}\")\n",
    "            if not os.path.exists(depth_path):\n",
    "                Image.new('L', (100, 100), color=128).save(depth_path) # Grayscale\n",
    "                print(f\"Created dummy Depth image: {depth_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create dummy image {i}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Apply the normalization patch for consistency with training\n",
    "    patch_normalize_preprocess()\n",
    "    print(\"Applied transformers.image_transforms.normalize patch.\")\n",
    "\n",
    "    # 1. Load Tokenizer and Image Processor (minimal parts of the model)\n",
    "    print(f\"Loading tokenizer and image_processor from {BASE_MODEL_PATH_FOR_CONFIG}...\")\n",
    "    try:\n",
    "        # We only need tokenizer and image_processor, not the full model weights for this test\n",
    "        # Temporarily load model just to get these, then delete model\n",
    "        # Use trust_remote_code=True if the model requires it (common for VILA/LLaVA based models)\n",
    "        tokenizer, model_temp, image_processor, _ = load_pretrained_model(\n",
    "            BASE_MODEL_PATH_FOR_CONFIG, model_name_or_path=None, model_base=None, \n",
    "            load_8bit=False, load_4bit=False, trust_remote_code=True \n",
    "        )\n",
    "        del model_temp # Free up memory\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        print(\"Tokenizer and Image Processor loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer/image_processor from {BASE_MODEL_PATH_FOR_CONFIG}: {e}\")\n",
    "        print(\"Please ensure the model path is correct and you have an internet connection if it's a Hugging Face ID.\")\n",
    "        return\n",
    "\n",
    "    # 2. Setup DataArguments\n",
    "    # Mimic DataArguments from llava.train.args or what AICityLazySpatialDataset expects\n",
    "    class MinimalDataArgs:\n",
    "        def __init__(self, processor):\n",
    "            self.image_processor = processor\n",
    "            self.image_aspect_ratio = 'pad'  # Critical: Test with 'pad' or 'resize'\n",
    "            self.is_multimodal = True\n",
    "            self.mm_use_im_start_end = False # As per 3_sft.sh\n",
    "            # Add any other attributes from DataArguments your dataset class or preprocess functions use\n",
    "            self.image_grid_pinpoints = None \n",
    "            self.use_rle_masks = True # Assuming your dataset class uses this\n",
    "            self.vflan_no_system_prompt = True # if your preprocess expects this\n",
    "\n",
    "    data_args = MinimalDataArgs(image_processor)\n",
    "\n",
    "    # Create dummy data if your test paths don't exist\n",
    "    create_dummy_data_if_not_exists()\n",
    "\n",
    "\n",
    "    # 3. Instantiate your AICityLazySpatialDataset\n",
    "    print(f\"\\nInstantiating AICityLazySpatialDataset with JSONL: {PROCESSED_JSONL_PATH}\")\n",
    "    try:\n",
    "        aicity_dataset = AICityLazySpatialDataset(\n",
    "            data_path=PROCESSED_JSONL_PATH,\n",
    "            rgb_image_folder=RGB_IMAGE_BASE_DIR,\n",
    "            depth_image_folder=DEPTH_IMAGE_BASE_DIR,\n",
    "            tokenizer=tokenizer,\n",
    "            data_args=data_args\n",
    "        )\n",
    "        print(f\"Dataset instantiated. Number of samples: {len(aicity_dataset)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR instantiating AICityLazySpatialDataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # 4. Inspect a Few Samples\n",
    "    if not aicity_dataset or len(aicity_dataset) == 0:\n",
    "        print(\"No samples in dataset to check.\")\n",
    "        return\n",
    "        \n",
    "    num_samples_to_check = min(3, len(aicity_dataset))\n",
    "    print(f\"\\nWill check {num_samples_to_check} samples...\")\n",
    "\n",
    "    for i in range(num_samples_to_check):\n",
    "        print(f\"\\n----------- Checking Sample Index: {i} -----------\")\n",
    "        try:\n",
    "            sample_data_dict = aicity_dataset[i] # Calls __getitem__\n",
    "            original_json_sample = aicity_dataset.list_data_dict[i]\n",
    "\n",
    "            print(f\"Original Sample ID from JSONL: {original_json_sample.get('id')}\")\n",
    "            print(f\"Original 'image_base_filename': {original_json_sample.get('image_base_filename')}\")\n",
    "            print(\"Sample Data Dictionary Keys:\", sample_data_dict.keys())\n",
    "\n",
    "            # Check input_ids and labels\n",
    "            input_ids = sample_data_dict['input_ids']\n",
    "            labels = sample_data_dict['labels']\n",
    "            print(f\"  input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}\")\n",
    "            \n",
    "            # Robust decode\n",
    "            decoded_input_ids = robust_decode_local(tokenizer, input_ids)\n",
    "            print(f\"  Decoded input_ids (snippet): {decoded_input_ids[:300]}...\") # First 300 chars\n",
    "            \n",
    "            # Check for <image>\\n and <mask> <depth> in the original conversation from JSONL\n",
    "            # as the decoded_input_ids will have IMAGE_TOKEN_INDEX resolved.\n",
    "            first_human_turn_jsonl = \"\"\n",
    "            for turn in original_json_sample.get(\"conversations\", []):\n",
    "                if turn.get(\"from\") == \"human\":\n",
    "                    first_human_turn_jsonl = turn.get(\"value\", \"\")\n",
    "                    break\n",
    "            \n",
    "            if not first_human_turn_jsonl.strip().startswith(DEFAULT_IMAGE_TOKEN + \"\\n\"):\n",
    "                print(f\"  WARNING (JSONL Check): First human turn in JSONL DOES NOT start with '{DEFAULT_IMAGE_TOKEN}\\\\n'. Found: '{first_human_turn_jsonl[:50]}...'\")\n",
    "            if \"<mask> <depth>\" not in first_human_turn_jsonl:\n",
    "                print(f\"  WARNING (JSONL Check): '<mask> <depth>' not found in human question from JSONL. Found: '{first_human_turn_jsonl[:100]}...'\")\n",
    "\n",
    "\n",
    "            print(f\"  labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "            valid_labels_indices = labels != IGNORE_INDEX\n",
    "            valid_labels = labels[valid_labels_indices]\n",
    "            decoded_valid_labels = robust_decode_local(tokenizer, valid_labels)\n",
    "            print(f\"  Decoded valid labels (snippet): {decoded_valid_labels[:200]}...\")\n",
    "\n",
    "            # Check image tensor\n",
    "            if sample_data_dict.get('image') is not None:\n",
    "                img_tensor = sample_data_dict['image']\n",
    "                print(f\"  image tensor shape: {img_tensor.shape}, dtype: {img_tensor.dtype}, min: {img_tensor.min():.2f}, max: {img_tensor.max():.2f}\")\n",
    "                assert img_tensor.ndim == 4 and img_tensor.shape[0] == 1 and img_tensor.shape[1] == 3, \"Image tensor shape error\"\n",
    "            else: print(\"  ERROR: image tensor is None!\")\n",
    "\n",
    "            # Check depths tensor\n",
    "            if sample_data_dict.get('depths') is not None:\n",
    "                depth_tensor = sample_data_dict['depths']\n",
    "                print(f\"  depths tensor shape: {depth_tensor.shape}, dtype: {depth_tensor.dtype}, min: {depth_tensor.min():.2f}, max: {depth_tensor.max():.2f}\")\n",
    "                assert depth_tensor.ndim == 4 and depth_tensor.shape[0] == 1 and depth_tensor.shape[1] == 3, \"Depth tensor shape error\"\n",
    "            else: print(\"  ERROR: depths tensor is None!\")\n",
    "\n",
    "            # Check masks tensor\n",
    "            if sample_data_dict.get('masks') is not None:\n",
    "                masks_tensor = sample_data_dict['masks']\n",
    "                print(f\"  masks tensor shape: {masks_tensor.shape}, dtype: {masks_tensor.dtype}, min: {masks_tensor.min():.2f}, max: {masks_tensor.max():.2f}\")\n",
    "                num_rle_original = len(original_json_sample.get('rle', []))\n",
    "                assert masks_tensor.shape[0] == num_rle_original, f\"Mask count mismatch: {masks_tensor.shape[0]} vs {num_rle_original}\"\n",
    "                assert masks_tensor.ndim == 3, \"Masks tensor shape error\" # [num_masks, H_proc, W_proc]\n",
    "            elif original_json_sample.get('rle'): print(\"  ERROR: masks tensor is None, but RLEs present in JSONL!\")\n",
    "            else: print(\"  masks tensor: None (no RLEs in JSONL)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during sample {i} (ID: {aicity_dataset.list_data_dict[i].get('id')}) check: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def robust_decode_local(tokenizer, token_ids_tensor):\n",
    "    \"\"\"Helper to decode token IDs, replacing known problematic IDs like -200.\"\"\"\n",
    "    if token_ids_tensor.ndim > 1: # Handle batched tensors if they appear\n",
    "        token_ids_tensor = token_ids_tensor.squeeze(0)\n",
    "    \n",
    "    cloned_ids = token_ids_tensor.clone().tolist() # Convert to list for easier manipulation\n",
    "    \n",
    "    # Replace IMAGE_TOKEN_INDEX (-200) with a placeholder string or pad token string for readability\n",
    "    # because even skip_special_tokens=True might error on raw -200 with some fast tokenizers.\n",
    "    # For decoding, we want to see if other text is fine.\n",
    "    final_ids_for_decode = []\n",
    "    for token_id in cloned_ids:\n",
    "        if token_id == -200: # IMAGE_TOKEN_INDEX\n",
    "            # Option 1: Skip (might be handled by skip_special_tokens if it doesn't error)\n",
    "            # continue \n",
    "            # Option 2: Replace with pad token ID, then skip_special_tokens will remove it.\n",
    "            final_ids_for_decode.append(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0)\n",
    "        elif token_id < 0 and token_id != IGNORE_INDEX: # Other unexpected negative IDs\n",
    "             final_ids_for_decode.append(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0)\n",
    "        else:\n",
    "            final_ids_for_decode.append(token_id)\n",
    "            \n",
    "    return tokenizer.decode(final_ids_for_decode, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0, len(aicity_dataset))\n",
    "sample_data_dict = aicity_dataset[i]\n",
    "if sample_data_dict.get('depths') is not None:\n",
    "    depth_to_show = sample_data_dict['depths'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "    # Assuming it's normalized to [0,1] by processor, or you might need to denormalize\n",
    "    # If it became 3-channel by duplicating, take one channel or average\n",
    "    if depth_to_show.shape[2] == 3:\n",
    "        depth_to_show = depth_to_show[:, :, 0] \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(sample_data_dict['image'][0].cpu().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(depth_to_show, cmap='gray')\n",
    "    plt.title(f\"Processed Depth - Sample {i}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
