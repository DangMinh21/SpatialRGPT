{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de25f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dangminh/Desktop/SpatialRGPT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be40ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dangminh/miniconda3/envs/spatialrgpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied transformers.image_transforms.normalize patch.\n"
     ]
    }
   ],
   "source": [
    "# Assuming transformer_normalize_monkey_patch.py is in llava/train/\n",
    "from llava.train.transformer_normalize_monkey_patch import patch_normalize_preprocess\n",
    "patch_normalize_preprocess() # Apply the patch\n",
    "print(\"Applied transformers.image_transforms.normalize patch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dangminh/miniconda3/envs/spatialrgpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'accelerate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Ensure you are in the SpatialRGPT directory for relative imports if not installed as a package\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Or add to sys.path if needed:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# import sys\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# sys.path.insert(0, '/content/SpatialRGPT') # Adjust if your SpatialRGPT clone is elsewhere\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataArguments \u001b[38;5;66;03m# Assuming this is how DataArguments is defined\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pretrained_model \u001b[38;5;66;03m# To get tokenizer and image_processor\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maicity_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AICityLazySpatialDataset \u001b[38;5;66;03m# Your custom class\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_MASK_TOKEN, DEFAULT_DEPTH_TOKEN\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaLlamaConfig, LlavaLlamaModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_mistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaMistralConfig, LlavaMistralForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_mixtral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaMixtralConfig, LlavaMixtralForCausalLM\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/language_model/llava_llama.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalLMOutputWithPast\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_llava\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaConfig\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_arch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaMetaForCausalLM, LlavaMetaModel\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vision_tower\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_projector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_mm_projector\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/llava_arch.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_llava\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaConfig\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_llm_and_tokenizer\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vision_tower\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_projector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_mm_projector\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregion_extractor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_region_extractor\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/builder.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, PretrainedConfig, PreTrainedModel\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPVisionTower, CLIPVisionTowerS2\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintern_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternVisionTower\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mradio_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RADIOVisionTower\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/clip_encoder.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPImageProcessor, CLIPVisionModel, PretrainedConfig\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisionTower, VisionTowerS2\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCLIPVisionTower\u001b[39;00m(VisionTower):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name_or_path: \u001b[38;5;28mstr\u001b[39m, config: PretrainedConfig):\n",
      "File \u001b[0;32m~/Desktop/SpatialRGPT/llava/model/multimodal_encoder/vision_encoder.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_hook_to_module\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01ms2wrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m forward \u001b[38;5;28;01mas\u001b[39;00m multiscale_forward\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, PreTrainedModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accelerate'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np # For potential visualization/debug\n",
    "import matplotlib.pyplot as plt # For visualization\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths on your Colab/RunPod instance\n",
    "PROCESSED_JSONL_PATH = \"PhysicalAI-Spatial-Intelligence-Warehouse/formatted_dataset/train_aicity_srgpt.jsonl\" # Path to your converted JSONL\n",
    "RGB_IMAGE_BASE_DIR = \"PhysicalAI-Spatial-Intelligence-Warehouse/train_sample/images\"      # Base path for AI City RGB images\n",
    "DEPTH_IMAGE_BASE_DIR = \"PhysicalAI-Spatial-Intelligence-Warehouse/train_sample/depths\"    # Base path for AI City Depth images\n",
    "SPATIAL_RGPT_MODEL_PATH = \"checkpoints/SpatialRGPT-VILA1.5-8B\" # For tokenizer & image_processor\n",
    "\n",
    "# Ensure you are in the SpatialRGPT directory for relative imports if not installed as a package\n",
    "# Or add to sys.path if needed:\n",
    "# import sys\n",
    "# sys.path.insert(0, '/content/SpatialRGPT') # Adjust if your SpatialRGPT clone is elsewhere\n",
    "\n",
    "from llava.train.args import DataArguments # Assuming this is how DataArguments is defined\n",
    "from llava.model.builder import load_pretrained_model # To get tokenizer and image_processor\n",
    "from llava.data.aicity_dataset import AICityLazySpatialDataset # Your custom class\n",
    "from llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_MASK_TOKEN, DEFAULT_DEPTH_TOKEN\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "\n",
    "# --- 1. Load Tokenizer and Image Processor (from the base model) ---\n",
    "print(f\"Loading tokenizer and image_processor from {SPATIAL_RGPT_MODEL_PATH}...\")\n",
    "model_name = get_model_name_from_path(SPATIAL_RGPT_MODEL_PATH)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=SPATIAL_RGPT_MODEL_PATH,\n",
    "    model_name=model_name, model_base=None, load_8bit=False, load_4bit=False\n",
    ")\n",
    "# We don't need the full 'model' here, just tokenizer and image_processor\n",
    "del model \n",
    "torch.cuda.empty_cache()\n",
    "print(\"Tokenizer and Image Processor loaded.\")\n",
    "\n",
    "# --- 2. Setup DataArguments ---\n",
    "# These arguments are usually passed via command line in train.py\n",
    "# We need to manually create a DataArguments object or a simple namespace\n",
    "class SimpleDataArgs:\n",
    "    def __init__(self):\n",
    "        self.image_processor = image_processor # Crucial: assign the loaded processor\n",
    "        self.image_aspect_ratio = 'pad' # Or 'resize', consistent with 3_sft.sh or desired setting. 'pad' is often safer.\n",
    "        self.is_multimodal = True\n",
    "        self.mm_use_im_start_end = False # Match 3_sft.sh\n",
    "        # Add other data_args if your AICityLazySpatialDataset or preprocess functions depend on them\n",
    "        # e.g. from the `train.py` script DataArguments dataclass\n",
    "        self.image_grid_pinpoints = None \n",
    "        self.use_rle_masks = True # Assuming you want to use RLEs\n",
    "        self.vflan_no_system_prompt = True # From 3_sft.sh, passed to preprocess\n",
    "\n",
    "data_args = SimpleDataArgs()\n",
    "\n",
    "# --- 3. Instantiate your AICityLazySpatialDataset ---\n",
    "print(f\"Instantiating AICityLazySpatialDataset with JSONL: {PROCESSED_JSONL_PATH}\")\n",
    "try:\n",
    "    aicity_dataset = AICityLazySpatialDataset(\n",
    "        data_path=PROCESSED_JSONL_PATH,\n",
    "        rgb_image_folder=RGB_IMAGE_BASE_DIR,\n",
    "        depth_image_folder=DEPTH_IMAGE_BASE_DIR,\n",
    "        tokenizer=tokenizer,\n",
    "        data_args=data_args\n",
    "    )\n",
    "    print(f\"Dataset instantiated. Number of samples: {len(aicity_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error instantiating dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    aicity_dataset = None\n",
    "\n",
    "# --- 4. Inspect a Few Samples ---\n",
    "if aicity_dataset:\n",
    "    num_samples_to_check = min(3, len(aicity_dataset))\n",
    "    if num_samples_to_check == 0:\n",
    "        print(\"No samples in the dataset to check.\")\n",
    "\n",
    "    for i in range(num_samples_to_check):\n",
    "        print(f\"\\n----------- Checking Sample Index: {i} -----------\")\n",
    "        try:\n",
    "            sample_data_dict = aicity_dataset[i] # This calls __getitem__\n",
    "            original_json_sample = aicity_dataset.list_data_dict[i] # Get the raw entry from JSONL\n",
    "\n",
    "            print(f\"Original Sample ID from JSONL: {original_json_sample.get('id')}\")\n",
    "            print(\"Sample Data Dictionary Keys:\", sample_data_dict.keys())\n",
    "\n",
    "            # Check input_ids and labels\n",
    "            print(f\"  input_ids shape: {sample_data_dict['input_ids'].shape}, dtype: {sample_data_dict['input_ids'].dtype}\")\n",
    "            decoded_input_ids = tokenizer.decode(sample_data_dict['input_ids'], skip_special_tokens=True)\n",
    "            print(f\"  Decoded input_ids (snippet): {decoded_input_ids[:500]}...\") # Print a longer snippet\n",
    "            \n",
    "            # Verify <image>\\n and <mask> <depth> substitution\n",
    "            if not decoded_input_ids.strip().startswith(DEFAULT_IMAGE_TOKEN + \"\\n\"):\n",
    "                print(f\"  WARNING: Decoded input_ids DO NOT start with '{DEFAULT_IMAGE_TOKEN}\\\\n'\")\n",
    "            if \"<mask> <depth>\" not in decoded_input_ids:\n",
    "                # This check might be tricky if <mask> <depth> are already tokenized to special tokens\n",
    "                # Instead, check the original conversation in the sample_data_dict if it went through preprocess_multimodal\n",
    "                original_human_q = \"\"\n",
    "                for conv_turn in original_json_sample['conversations']:\n",
    "                    if conv_turn['from'] == 'human':\n",
    "                        original_human_q = conv_turn['value']\n",
    "                        break\n",
    "                if \"<mask> <depth>\" not in original_human_q: # This checks pre-tokenization from your JSONL\n",
    "                     print(f\"  WARNING: '<mask> <depth>' not found in human question from JSONL: {original_human_q[:100]}\")\n",
    "\n",
    "\n",
    "            print(f\"  labels shape: {sample_data_dict['labels'].shape}, dtype: {sample_data_dict['labels'].dtype}\")\n",
    "            valid_labels_indices = sample_data_dict['labels'] != IGNORE_INDEX\n",
    "            valid_labels = sample_data_dict['labels'][valid_labels_indices]\n",
    "            print(f\"  Decoded valid labels (snippet): {tokenizer.decode(valid_labels[:50], skip_special_tokens=True)}...\")\n",
    "\n",
    "            # Check image tensor\n",
    "            if sample_data_dict['image'] is not None:\n",
    "                print(f\"  image tensor shape: {sample_data_dict['image'].shape}, dtype: {sample_data_dict['image'].dtype}\")\n",
    "                assert sample_data_dict['image'].ndim == 4, \"Image tensor should be [1, C, H, W]\"\n",
    "                assert sample_data_dict['image'].shape[1] == 3, \"Image tensor should have 3 channels\"\n",
    "            else:\n",
    "                print(\"  ERROR: image tensor is None!\")\n",
    "\n",
    "            # Check depths tensor\n",
    "            if sample_data_dict.get('depths') is not None:\n",
    "                print(f\"  depths tensor shape: {sample_data_dict['depths'].shape}, dtype: {sample_data_dict['depths'].dtype}\")\n",
    "                assert sample_data_dict['depths'].ndim == 4, \"Depth tensor should be [1, C, H, W]\"\n",
    "                # For SigLIP/CLIP processors, depth is usually converted to 3 channels.\n",
    "                assert sample_data_dict['depths'].shape[1] == 3, \"Depth tensor should have 3 channels after processing\"\n",
    "            else:\n",
    "                print(\"  ERROR: depths tensor is None!\")\n",
    "\n",
    "            # Check masks tensor\n",
    "            if sample_data_dict.get('masks') is not None:\n",
    "                print(f\"  masks tensor shape: {sample_data_dict['masks'].shape}, dtype: {sample_data_dict['masks'].dtype}\")\n",
    "                num_rle_original = len(original_json_sample.get('rle', []))\n",
    "                assert sample_data_dict['masks'].shape[0] == num_rle_original, \\\n",
    "                    f\"Mismatch in mask count: tensor has {sample_data_dict['masks'].shape[0]}, RLEs in source: {num_rle_original}\"\n",
    "                assert sample_data_dict['masks'].ndim == 3, \"Masks tensor should be [num_masks, H_proc, W_proc]\"\n",
    "            elif original_json_sample.get('rle'): # If RLEs were present but masks tensor is None\n",
    "                 print(\"  ERROR: masks tensor is None, but RLEs were present in the source JSONL!\")\n",
    "            else: # No RLEs in source, so masks tensor being None is OK\n",
    "                print(\"  masks tensor: None (as expected, no RLEs in source JSONL for this sample)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR checking sample {i} (ID: {aicity_dataset.list_data_dict[i].get('id')}): {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0, len(aicity_dataset))\n",
    "sample_data_dict = aicity_dataset[i]\n",
    "if sample_data_dict.get('depths') is not None:\n",
    "    depth_to_show = sample_data_dict['depths'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "    # Assuming it's normalized to [0,1] by processor, or you might need to denormalize\n",
    "    # If it became 3-channel by duplicating, take one channel or average\n",
    "    if depth_to_show.shape[2] == 3:\n",
    "        depth_to_show = depth_to_show[:, :, 0] \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(sample_data_dict['image'][0].cpu().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(depth_to_show, cmap='gray')\n",
    "    plt.title(f\"Processed Depth - Sample {i}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialrgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
