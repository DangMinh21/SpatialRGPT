{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b220669-6a8f-4436-a6da-d887309263d5",
   "metadata": {},
   "source": [
    "## RegionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3ce551a-8d1a-4109-8060-bc028c8a3f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152])\n",
      "torch.Size([1, 5, 1152])\n",
      "torch.Size([1, 5, 1152])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class RegionTransformer(nn.Module):\n",
    "    def __init__(self, d_model=1152, nhead=8, num_self_layers=6, num_cross_layers=4, max_regions=15, max_tokens_img=800):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_embed_region = nn.Parameter(torch.randn(1, 2 * max_regions + 1, d_model))\n",
    "        self.pos_embed_image = nn.Parameter(torch.randn(1, max_tokens_img, d_model))\n",
    "\n",
    "        # Transformer encoders\n",
    "        self.region_encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True),\n",
    "            num_layers=num_self_layers\n",
    "        )\n",
    "        self.cross_decoder = TransformerDecoder(\n",
    "            TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True),\n",
    "            num_layers=num_cross_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, Freg_rgb, Freg_depth, Fimg_rgb):\n",
    "        B, Nr, F = Freg_rgb.shape\n",
    "        _, Nt, _ = Fimg_rgb.shape\n",
    "\n",
    "        # Concatenate region features + CLS token\n",
    "        Freg = torch.cat([Freg_rgb, Freg_depth], dim=1)       # (B, 2*Nr, F)\n",
    "        cls = self.cls_token.expand(B, 1, F)                  # (B, 1, F)\n",
    "        Freg = torch.cat([cls, Freg], dim=1)                  # (B, 2*Nr+1, F)\n",
    "        Freg += self.pos_embed_region[:, :Freg.size(1), :]    # Add pos emb; Freg.size(1) maybe < max-ntokens\n",
    "\n",
    "        # Region encoder\n",
    "        Freg_encoded = self.region_encoder(Freg)              # (B, 2*Nr+1, F)\n",
    "\n",
    "        # Add position embedding for image features\n",
    "        Fimg_rgb_pos = Fimg_rgb + self.pos_embed_image[:, :Nt, :]  # (B, Nt, F)\n",
    "\n",
    "        # Cross attention\n",
    "        X = self.cross_decoder(Freg_encoded, Fimg_rgb_pos)    # (B, 2*Nr+1, F)\n",
    "\n",
    "        CLS = X[:, 0, :]                                      # (B, F)\n",
    "        region_tokens = X[:, 1:, :].chunk(2, dim=1)           # [(B, Nr, F), (B, Nr, F)]\n",
    "        Freg_rgb_trans, Freg_depth_trans = region_tokens\n",
    "\n",
    "        return CLS, Freg_rgb_trans, Freg_depth_trans\n",
    "\n",
    "Freg_rgb = torch.randn(1, 5, 1152)\n",
    "Freg_depth = torch.randn(1, 5, 1152)\n",
    "Fimg_rgb = torch.randn(1, 792, 1152)\n",
    "\n",
    "model = RegionTransformer()\n",
    "CLS, Freg_rgb_trans, Freg_depth_trans = model(Freg_rgb, Freg_depth, Fimg_rgb)\n",
    "\"\"\"\n",
    "CLS: cho bài toán phân loại/hồi quy từ đặc trưng gobal của regions\n",
    "Freg_rgb_trans, Freg_depth_trans: cho qua RGBProjector và DepthProjector\n",
    "\"\"\"\n",
    "print(CLS.shape)\n",
    "print(Freg_rgb_trans.shape)\n",
    "print(Freg_depth_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86cd48-66db-423b-82d8-7d731aaadbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "444117a2-62a8-4278-90be-c2e15036b057",
   "metadata": {},
   "source": [
    "## Original RegionExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4645278a-d339-4fa3-a93b-ed2d6a652dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rgb region:   (2, torch.Size([5, 4096]), torch.Size([3, 4096]))\n",
      "For depth region: (2, torch.Size([5, 4096]), torch.Size([3, 4096]))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel, PretrainedConfig, PreTrainedModel\n",
    "\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskPooling(nn.Module):\n",
    "    def __init__(self, mask_threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.mask_threshold = mask_threshold\n",
    "\n",
    "    def forward(self, x, mask_list, return_list=False, return_mask=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, (HW), C]\n",
    "            mask_list: List( tensor[M, IH, IW] )\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        if mask_list is None:\n",
    "            mask_list = [None for i in range(batch_size)]\n",
    "\n",
    "        output = []\n",
    "        attn_mask_list = []\n",
    "        for i in range(batch_size):\n",
    "            x_len = x.size(1)\n",
    "            mask = mask_list[i]\n",
    "            if mask is None:\n",
    "                output.append(None)\n",
    "                attn_mask_list.append(None)\n",
    "            else:\n",
    "                # resize mask from image shape to feature map shape\n",
    "                mask_hw = mask.size(-1) * mask.size(-2)\n",
    "                scale_factor = (x_len / mask_hw) ** 0.5\n",
    "\n",
    "                mask = mask.detach()\n",
    "                mask = mask.float()[None, ...]\n",
    "                mask = nn.functional.interpolate(mask, scale_factor=scale_factor, mode=\"bilinear\")\n",
    "                mask = mask.to(x.dtype)\n",
    "                mask = mask[0]\n",
    "                feature = x[i]\n",
    "\n",
    "                denorm = mask.sum(dim=(-1, -2)) + 1e-8  # M\n",
    "                denorm = denorm.unsqueeze(-1)  # M, 1\n",
    "\n",
    "                mask = mask.flatten(start_dim=1)  # M, H, W -> M, HW\n",
    "\n",
    "                attn_mask_list.append((mask > self.mask_threshold).to(mask.dtype))  # M, HW\n",
    "\n",
    "                mask_pooled_x = torch.einsum(\n",
    "                    \"lc,ml->mc\",\n",
    "                    feature,\n",
    "                    mask / denorm,\n",
    "                )\n",
    "                # mc output\n",
    "                output.append(mask_pooled_x)\n",
    "\n",
    "        if return_list:\n",
    "            if return_mask:\n",
    "                return output, attn_mask_list\n",
    "            return output\n",
    "        else:\n",
    "            # FIXME: Not support Nonetype\n",
    "            output = torch.cat(output)\n",
    "            return output\n",
    "\n",
    "\n",
    "def get_feature_refinement_module(vision_hidden_size, feature_refinement_type=\"deconv2x\"):\n",
    "    deconv_match = re.match(r\"^deconv(\\d+)x$\", feature_refinement_type)\n",
    "    if deconv_match:\n",
    "        deconv_depth = int(deconv_match.group(1))\n",
    "        modules = []\n",
    "        for i in range(deconv_depth - 1):\n",
    "            modules.append(nn.ConvTranspose2d(vision_hidden_size, vision_hidden_size, kernel_size=2, stride=2))\n",
    "            modules.append(LayerNorm2d(vision_hidden_size))\n",
    "            modules.append(nn.GELU())\n",
    "        modules.append(nn.ConvTranspose2d(vision_hidden_size, vision_hidden_size, kernel_size=2, stride=2))\n",
    "        modules.append(nn.GELU())\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    raise ValueError(f\"Unknown feature refinement type: {feature_refinement_type}\")\n",
    "\n",
    "\n",
    "class RegionExtractorConfig(PretrainedConfig):\n",
    "    model_type = \"region_extractor\"\n",
    "\n",
    "    def __init__(self, region_extractor_type: str = None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.region_extractor_type = region_extractor_type\n",
    "\n",
    "\n",
    "class RegionExtractor(PreTrainedModel):\n",
    "    config_class = RegionExtractorConfig\n",
    "\n",
    "    def __init__(self, region_extractor_cfg: RegionExtractorConfig, config: PretrainedConfig):\n",
    "        super().__init__(region_extractor_cfg)\n",
    "        region_extractor_type = region_extractor_cfg.region_extractor_type\n",
    "\n",
    "        if region_extractor_type == \"regiongpt\":\n",
    "            self.mask_pooling = MaskPooling()\n",
    "            self.feature_refinement_module = get_feature_refinement_module(config.mm_hidden_size)\n",
    "            # TODO: hardcoded pooling size here, should be inside cfg\n",
    "            self.ada_pooling = nn.AdaptiveAvgPool2d(27)\n",
    "            self.rgb_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "            self.depth_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "        elif region_extractor_type == \"duplicate\":\n",
    "            self.mask_pooling = MaskPooling()\n",
    "            self.rgb_projector = None\n",
    "            self.depth_projector = None\n",
    "        elif region_extractor_type == \"duplicate_deconv\":\n",
    "            self.feature_refinement_module = get_feature_refinement_module(config.mm_hidden_size)\n",
    "            self.ada_pooling = nn.AdaptiveAvgPool2d(27)\n",
    "            self.mask_pooling = MaskPooling()\n",
    "            self.rgb_projector = None\n",
    "            self.depth_projector = None\n",
    "\n",
    "    def feature_refinement(self, tower_features):\n",
    "        HW = tower_features.shape[1]\n",
    "        tower_features = einops.rearrange(tower_features, \"N (H W) C -> N C H W\", H=int(HW**0.5))\n",
    "        hres_tower_features = self.feature_refinement_module(tower_features)\n",
    "        # local feature branch\n",
    "        hres_tower_features_flatten = einops.rearrange(hres_tower_features, \"N C H W -> N (H W) C\")\n",
    "\n",
    "        # global feature branch\n",
    "        ada_image_feature = self.ada_pooling(hres_tower_features)\n",
    "        lres_tower_features_flatten = einops.rearrange(ada_image_feature, \"N C H W -> N (H W) C\")\n",
    "        return hres_tower_features_flatten, lres_tower_features_flatten\n",
    "\n",
    "    def extract_region_features(self, hres_tower_features, masks, connector):\n",
    "        # assume is already flattened -> 'N (H W) C'\n",
    "        if self.config.region_extractor_type == \"regiongpt\":\n",
    "            mask_embeds = self.mask_pooling(hres_tower_features, masks, return_list=True)\n",
    "            _mask_embeds = []\n",
    "            for mask_embed in mask_embeds:\n",
    "                if mask_embed is None:\n",
    "                    _mask_embeds.append(None)\n",
    "                else:\n",
    "                    _mask_embeds.append(\n",
    "                        connector(mask_embed)\n",
    "                    )\n",
    "\n",
    "        elif self.config.region_extractor_type in [\"duplicate\", \"duplicate_deconv\"]:\n",
    "            raise NotImplementedError(f\"{self.config.region_extractor_type} not implemented\")\n",
    "\n",
    "        mask_embeds = _mask_embeds\n",
    "\n",
    "        return mask_embeds\n",
    "\n",
    "    def forward(self, \n",
    "                image_features, # H\n",
    "                depth_features,\n",
    "                masks,\n",
    "                *args, **kwargs):\n",
    "        \n",
    "        mask_embeds = self.extract_region_features(image_features, masks, self.rgb_projector)\n",
    "        if depth_features is not None:\n",
    "            depth_embeds = self.extract_region_features(depth_features, masks, self.depth_projector)\n",
    "        else:\n",
    "            depth_embeds = None\n",
    "        return mask_embeds, depth_embeds\n",
    "        \n",
    "# --------------------------------------\n",
    "# assume batch_size = 2\n",
    "# input: rgb_image, depth_image, masks \n",
    "# output: rgb_region_emb, depth_region_emb\n",
    "Fimg_rgb = torch.randn(2, 729, 1152) # (B, L, F)    # Output of visusal encoder\n",
    "Fimg_depth = torch.randn(2, 729, 1152) # (B, L, F)  # Output of visual encoder\n",
    "masks = [                                           # (B, n_masks, W, H)   # Masks\n",
    "            torch.randn(5, 384, 384),               # 5 masks of sample 1 (assume batch size 2)\n",
    "            torch.randn(3, 384, 384)                # 3 masks of sample 2 (assume batch size 2)\n",
    "        ] \n",
    "\n",
    "# region_extractor model\n",
    "region_extractor_cfg = RegionExtractorConfig(region_extractor_type=\"regiongpt\")\n",
    "config = PretrainedConfig()\n",
    "config.mm_hidden_size = 1152\n",
    "config.hidden_size = 4096\n",
    "\n",
    "region_extractor = RegionExtractor(region_extractor_cfg, config)\n",
    "\n",
    "# forward\n",
    "hres_tower_features, lres_tower_features = region_extractor.feature_refinement(Fimg_rgb)       # get high feature and low feature\n",
    "# hres_tower_features: (B, 11664, 1152)\n",
    "# lres_tower_features: (B, 729, 1152)\n",
    "\n",
    "\n",
    "reg_rgb_emb, reg_depth_emb = region_extractor(hres_tower_features, Fimg_depth, masks) # mask_pooling + projector\n",
    "# reg_rgb_emb: list( tensor[n_masks, F])\n",
    "# reg_depth_emb: list( tensor[n_masks, F])\n",
    "\n",
    "print(f\"For rgb region:   {len(reg_rgb_emb), reg_rgb_emb[0].shape, reg_rgb_emb[1].shape}\") # (B, n_masks, F)\n",
    "print(f\"For depth region: {len(reg_depth_emb), reg_depth_emb[0].shape, reg_depth_emb[1].shape}\") # (B, n_masks, F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f896cc0-9181-4414-9b23-e26659a5cf5d",
   "metadata": {},
   "source": [
    "## Modify RegionExtractor to add RegionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ee3482be-0777-4591-a1cf-98b68a2d327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hres_tower_features: torch.Size([2, 11664, 1152])\n",
      "lres_tower_features: torch.Size([2, 729, 1152])\n",
      "CLS: torch.Size([2, 1, 1152])\n",
      "Freg_rgb_trans: len 2 - torch.Size([1, 5, 1152]) - torch.Size([1, 3, 1152])\n",
      "Freg_rgb_trans: len 2 - torch.Size([1, 5, 1152]) - torch.Size([1, 3, 1152])\n",
      "Freg_rgb_trans_proj: len 2 - torch.Size([1, 5, 4096]) - torch.Size([1, 3, 4096])\n",
      "Freg_rgb_trans_proj: len 2 - torch.Size([1, 5, 4096]) - torch.Size([1, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "# class RegionExtractor(PreTrainedModel):\n",
    "#     config_class = RegionExtractorConfig\n",
    "\n",
    "#     def __init__(self, region_extractor_cfg: RegionExtractorConfig, config: PretrainedConfig):\n",
    "#         super().__init__(region_extractor_cfg)\n",
    "#         region_extractor_type = region_extractor_cfg.region_extractor_type\n",
    "\n",
    "#         if region_extractor_type == \"regiongpt\":\n",
    "#             self.mask_pooling = MaskPooling()\n",
    "#             self.feature_refinement_module = get_feature_refinement_module(config.mm_hidden_size)\n",
    "#             # TODO: hardcoded pooling size here, should be inside cfg\n",
    "#             self.ada_pooling = nn.AdaptiveAvgPool2d(27)\n",
    "#             self.rgb_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "#             self.depth_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "            \n",
    "#             # ============= Add Region Transformer ============\n",
    "#             self.use_region_transformer = True\n",
    "#             self.Region_Transformer = RegionTransformer(d_model=1152, nhead=8, num_self_layers=6, num_cross_layers=4, max_regions=15, max_tokens_img=800)\n",
    "#             # =================================================\n",
    "            \n",
    "#         elif region_extractor_type == \"duplicate\":\n",
    "#             self.mask_pooling = MaskPooling()\n",
    "#             self.rgb_projector = None\n",
    "#             self.depth_projector = None\n",
    "#         elif region_extractor_type == \"duplicate_deconv\":\n",
    "#             self.feature_refinement_module = get_feature_refinement_module(config.mm_hidden_size)\n",
    "#             self.ada_pooling = nn.AdaptiveAvgPool2d(27)\n",
    "#             self.mask_pooling = MaskPooling()\n",
    "#             self.rgb_projector = None\n",
    "#             self.depth_projector = None\n",
    "\n",
    "#     def feature_refinement(self, tower_features):\n",
    "#         HW = tower_features.shape[1]\n",
    "#         tower_features = einops.rearrange(tower_features, \"N (H W) C -> N C H W\", H=int(HW**0.5))\n",
    "#         hres_tower_features = self.feature_refinement_module(tower_features)\n",
    "#         # local feature branch\n",
    "#         hres_tower_features_flatten = einops.rearrange(hres_tower_features, \"N C H W -> N (H W) C\")\n",
    "\n",
    "#         # global feature branch\n",
    "#         ada_image_feature = self.ada_pooling(hres_tower_features)\n",
    "#         lres_tower_features_flatten = einops.rearrange(ada_image_feature, \"N C H W -> N (H W) C\")\n",
    "#         return hres_tower_features_flatten, lres_tower_features_flatten\n",
    "\n",
    "#     def extract_region_features(self, hres_tower_features, masks, connector):\n",
    "#         # assume is already flattened -> 'N (H W) C'\n",
    "#         if self.config.region_extractor_type == \"regiongpt\":\n",
    "#             mask_embeds = self.mask_pooling(hres_tower_features, masks, return_list=True)\n",
    "#             _mask_embeds = []\n",
    "#             for mask_embed in mask_embeds:\n",
    "#                 if mask_embed is None:\n",
    "#                     _mask_embeds.append(None)\n",
    "#                 else:\n",
    "#                     # ----------- modify -----------\n",
    "#                     if not self.use_region_transformer:\n",
    "#                         _mask_embeds.append(connector(mask_embed))\n",
    "#                     else:\n",
    "#                         _mask_embeds.append(mask_embed) # remove connector, later\n",
    "#                     # ------------------------------\n",
    "\n",
    "#         elif self.config.region_extractor_type in [\"duplicate\", \"duplicate_deconv\"]:\n",
    "#             raise NotImplementedError(f\"{self.config.region_extractor_type} not implemented\")\n",
    "\n",
    "#         mask_embeds = _mask_embeds\n",
    "\n",
    "#         return mask_embeds\n",
    "\n",
    "#     def forward(self,\n",
    "#                 lres_tower_features, # add for region cross_decoder\n",
    "#                 hres_tower_features, \n",
    "#                 depth_features,\n",
    "#                 masks,\n",
    "#                 *args, **kwargs\n",
    "#                ):\n",
    "        \n",
    "#         Freg_rgbs = self.extract_region_features(hres_tower_features, masks, self.rgb_projector)      # rgb_region_emb list( tensor[n_masks, 1152] )\n",
    "#         if depth_features is not None:\n",
    "#             Freg_depths = self.extract_region_features(depth_features, masks, self.depth_projector)   # depth_region_emb list( tensor[n_masks, 1152] )\n",
    "#         else:\n",
    "#             Freg_depths = None\n",
    "#         if not self.use_region_transformer:\n",
    "#             return (None, None, None, Freg_rgbs, Freg_depths)\n",
    "            \n",
    "#         else:\n",
    "#         # ========== Modify forward when using RegionTransformer ==========\n",
    "#             CLS = []\n",
    "#             Freg_rgb_trans = []\n",
    "#             Freg_depth_trans = []\n",
    "#             Freg_rgb_tran_projs = []\n",
    "#             Freg_depth_tran_projs = []\n",
    "                \n",
    "#             for Freg_rgb, Freg_depth, lres_tower_feature in zip(Freg_rgbs, Freg_depths, lres_tower_features):\n",
    "#                 # print(f\"Freg_rgb: {Freg_rgb[None].shape}\")\n",
    "#                 # print(f\"Freg_depth: {Freg_depth[None].shape}\")\n",
    "#                 # print(f\"lres_tower_feature: {lres_tower_feature[None].shape}\")\n",
    "#                 Cls, Freg_rgb_tran, Freg_depth_tran = self.Region_Transformer(Freg_rgb[None], Freg_depth[None], lres_tower_feature[None])\n",
    "                \n",
    "#                 Freg_rgb_tran_proj = self.rgb_projector(Freg_rgb_tran)\n",
    "#                 Freg_depth_tran_proj = self.depth_projector(Freg_depth_tran)\n",
    "\n",
    "#                 CLS.append(Cls)\n",
    "#                 Freg_rgb_trans.append(Freg_rgb_tran)\n",
    "#                 Freg_depth_trans.append(Freg_depth_tran)\n",
    "#                 Freg_rgb_tran_projs.append(Freg_rgb_tran_proj)\n",
    "#                 Freg_depth_tran_projs.append(Freg_depth_tran_proj)\n",
    "            \n",
    "#             # return mask_embeds, depth_embeds\n",
    "#             return (torch.stack(CLS),         # CLS embed for each sample\n",
    "#                     Freg_rgb_trans,           # rgb region embeds before rgb projector\n",
    "#                     Freg_depth_trans,         # depth region embeds before rgb projector\n",
    "#                     Freg_rgb_tran_projs,      # rgb region embeds after rgb projector\n",
    "#                     Freg_depth_tran_projs)    # depth region embeds after rgb projector\n",
    "\n",
    "# --------------------------------------\n",
    "# assume batch_size = 2\n",
    "# input: rgb_image, depth_image, masks \n",
    "# output: rgb_region_emb, depth_region_emb\n",
    "Fimg_rgb = torch.randn(2, 729, 1152) # (B, L, F)    # Output of visusal encoder\n",
    "Fimg_depth = torch.randn(2, 729, 1152) # (B, L, F)  # Output of visual encoder\n",
    "masks = [                                           # (B, n_masks, W, H)\n",
    "            torch.randn(5, 384, 384),               # 5 masks of sample 1 (assume batch size 2)\n",
    "            torch.randn(3, 384, 384)                # 3 masks of sample 2 (assume batch size 2)\n",
    "        ] \n",
    "\n",
    "# # Config model\n",
    "# region_extractor_cfg = RegionExtractorConfig(region_extractor_type=\"regiongpt\")\n",
    "# config = PretrainedConfig()\n",
    "# config.mm_hidden_size = 1152\n",
    "# config.hidden_size = 4096\n",
    "\n",
    "# # Init model\n",
    "# region_extractor = RegionExtractor(region_extractor_cfg, config) \n",
    "\n",
    "# # output\n",
    "# hres_tower_features, lres_tower_features = region_extractor.feature_refinement(Fimg_rgb)       # get high features and low features\n",
    "# print(f\"hres_tower_features: {hres_tower_features.shape}\")\n",
    "# print(f\"lres_tower_features: {lres_tower_features.shape}\")\n",
    "# # hres_tower_features: (B, 11664, 1152)\n",
    "# # lres_tower_features: (B, 729, 1152)\n",
    "\n",
    "# # reg_rgb_emb, reg_depth_emb = region_extractor(hres_tower_features, Fimg_depth, masks) # original\n",
    "# (CLS, \n",
    "#  Freg_rgb_trans,\n",
    "#  Freg_depth_trans,\n",
    "#  Freg_rgb_embs,\n",
    "#  Freg_depth_embs) = region_extractor(lres_tower_features,   # add low feature for region cross_decoder\n",
    "#                                      hres_tower_features,\n",
    "#                                      Fimg_depth,\n",
    "#                                      masks) \n",
    "# print(f\"CLS: {CLS.shape}\")\n",
    "# print(f\"Freg_rgb_trans: len {len(Freg_rgb_trans)} - {Freg_rgb_trans[0].shape} - {Freg_rgb_trans[1].shape}\")         # output of RegionTransformer for rgb region\n",
    "# print(f\"Freg_rgb_trans: len {len(Freg_depth_trans)} - {Freg_depth_trans[0].shape} - {Freg_depth_trans[1].shape}\")   # output of RegionTransformer for depth region\n",
    "# print(f\"Freg_rgb_trans_proj: len {len(Freg_rgb_embs)} - {Freg_rgb_embs[0].shape} - {Freg_rgb_embs[1].shape}\")       # output of rgb_projector for rgb region\n",
    "# print(f\"Freg_rgb_trans_proj: len {len(Freg_depth_embs)} - {Freg_depth_embs[0].shape} - {Freg_depth_embs[1].shape}\") # output of depth_projector for rgb region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f98a20-6f3d-4496-a46f-69cdf5c08f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Defining Inputs & Config ---\n",
      "Simulated Fimg_rgb (global patches): torch.Size([2, 729, 1152])\n",
      "Simulated Fimg_depth (global patches): torch.Size([2, 729, 1152])\n",
      "Simulated masks_list: 2 items, first item shape: torch.Size([5, 384, 384]), \n",
      "\t\t\t\tsecond item shape: torch.Size([3, 384, 384])\n",
      "\n",
      "--- 2. Instantiate Modules ---\n",
      "Modules instantiated successfully.\n",
      "\n",
      "--- 3. Simulating Forward Pass Data Flow ---\n",
      "\n",
      "--- Step A: Feature Pooling (Simulating RegionExtractor) ---\n",
      "hres_tower_features_rgb (for RGB mask pooling): torch.Size([2, 11664, 1152])\n",
      "lres_tower_features_rgb (for global projector): torch.Size([2, 729, 1152])\n",
      "Fimg_depth_patches (for depth mask pooling): torch.Size([2, 729, 1152])\n",
      "Unprojected RGB region features (sample 0 - 1): (torch.Size([5, 1152]), torch.Size([3, 1152]))\n",
      "Unprojected Depth region features (sample 0): (torch.Size([5, 1152]), torch.Size([3, 1152]))\n",
      "\n",
      "--- Step B: Region Interaction (RegionFeatureExtractor) ---\n",
      "Enhanced region features (sample 0): torch.Size([10, 1152])\n",
      "\n",
      "--- Step C: Branching for LLM and Auxiliary Heads ---\n",
      "\n",
      "  --- Branch 1 (LLM Pathway) ---\n",
      "Enhanced RGB region features (sample 0): torch.Size([5, 1152])\n",
      "Enhanced Depth region features (sample 0): torch.Size([5, 1152])\n",
      "Projected RGB for LLM (sample 0): torch.Size([5, 4096])\n",
      "Projected Depth for LLM (sample 0): torch.Size([5, 4096])\n",
      "\n",
      "  --- Branch 2 (Auxiliary Heads) ---\n",
      "Region Classifier inputs (sample 0): torch.Size([10, 1152])\n",
      "Region Classifier logits (sample 0): torch.Size([5, 10])\n",
      "Distance Head inputs (sample 0): tuple of 2 - for RGB: torch.Size([2, 1152]) - for Depth: torch.Size([2, 1152])\n",
      "Distance Head prediction (sample 0): 0.24084824323654175\n",
      "Left/Right Head inputs (sample 0): tuple of 2 - for RGB: torch.Size([2, 1152]) - for Depth: torch.Size([2, 1152])\n",
      "Left/Right Head logits (sample 0): tensor([-0.1226,  0.1650], grad_fn=<SqueezeBackward1>)\n",
      "MultipleChoice Head inputs (sample 0): tuple of 2 - for RGB: torch.Size([5, 1152]) - for Depth: torch.Size([5, 1152])\n",
      "MultipleChoice Head logits (sample 0): tensor([-0.1161, -0.1154, -0.1120, -0.1189, -0.1098],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Counting Head inputs (sample 0): tuple of 2 - for RGB: torch.Size([5, 1152]) - for Depth: torch.Size([5, 1152])\n",
      "Counting Head logits (sample 0): tensor([-0.0229, -0.0239, -0.0272, -0.0175, -0.0212],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "--- Data Flow Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "# flow in prepare_inputs_labels_for_multimodal() function\n",
    "\n",
    "from llava.model.region_extractor.base_extractor import RegionExtractor, RegionExtractorConfig\n",
    "from llava.model.region_transformer import RegionFeatureExtractor\n",
    "from llava.model.region_heads import RegionClassifier, DistanceHead, MultipleChoiceHead, CountingHead, LeftRightHead\n",
    "\n",
    "\n",
    "print(\"\\n--- 1. Defining Inputs & Config ---\")\n",
    "# --- Simulation Parameters ---\n",
    "BATCH_SIZE = 2\n",
    "NUM_MASKS_SAMPLE_1 = 5\n",
    "NUM_MASKS_SAMPLE_2 = 3\n",
    "TOTAL_MASKS = NUM_MASKS_SAMPLE_1 + NUM_MASKS_SAMPLE_2\n",
    "GLOBAL_IMG_PATCHES = 729 # (e.g., 27x27 grid from ada_pooling in original RegionExtractor)\n",
    "MM_HIDDEN_SIZE = 1152 # Feature dimension from Vision Tower / RegionExtractor\n",
    "LLM_HIDDEN_SIZE = 4096 # Feature dimension for the LLM\n",
    "\n",
    "# --- Simulated Tensors ---\n",
    "# These represent the outputs from the Vision Tower\n",
    "Fimg_rgb_patches = torch.randn(BATCH_SIZE, GLOBAL_IMG_PATCHES, MM_HIDDEN_SIZE)\n",
    "Fimg_depth_patches = torch.randn(BATCH_SIZE, GLOBAL_IMG_PATCHES, MM_HIDDEN_SIZE)\n",
    "\n",
    "# This represents the list of binary masks from the dataloader\n",
    "masks_list = [\n",
    "    torch.rand(NUM_MASKS_SAMPLE_1, 384, 384), # 5 masks for sample 1\n",
    "    torch.rand(NUM_MASKS_SAMPLE_2, 384, 384)  # 3 masks for sample 2\n",
    "]\n",
    "\n",
    "print(f\"Simulated Fimg_rgb (global patches): {Fimg_rgb_patches.shape}\")\n",
    "print(f\"Simulated Fimg_depth (global patches): {Fimg_depth_patches.shape}\")\n",
    "print(f\"Simulated masks_list: {len(masks_list)} items, first item shape: {masks_list[0].shape}, \\n\\t\\t\\t\\tsecond item shape: {masks_list[1].shape}\")\n",
    "\n",
    "\n",
    "# --- Model Configurations ---\n",
    "region_extractor_cfg = RegionExtractorConfig(region_extractor_type=\"regiongpt\")\n",
    "config = PretrainedConfig()\n",
    "config.mm_hidden_size = MM_HIDDEN_SIZE\n",
    "config.hidden_size = LLM_HIDDEN_SIZE # LLM dimension\n",
    "\n",
    "print(\"\\n--- 2. Instantiate Modules ---\")\n",
    "# --- Instantiate Original and New Modules ---\n",
    "# This is the original module responsible for pooling features from masks\n",
    "region_extractor = RegionExtractor(region_extractor_cfg, config).eval()\n",
    "\n",
    "region_feature_extractor_new = RegionFeatureExtractor(\n",
    "    dim=MM_HIDDEN_SIZE, # Operates on 1152-dim features\n",
    "    num_heads=8,        \n",
    "    num_transformer_layers=6, \n",
    "    num_cross_attn_layers=1   \n",
    ").eval()\n",
    "\n",
    "# initialize head\n",
    "# RegionHead for classify region\n",
    "num_region_classes = 10 # Example: pallet, shelf, transporter, etc.\n",
    "max_object_count = 15   # Example: max number of objects to count in a scene\n",
    "region_head = RegionClassifier(\n",
    "    infeatures=MM_HIDDEN_SIZE, # Takes the 1152-dim features\n",
    "    nclasses=num_region_classes,\n",
    ").eval()\n",
    "\n",
    "# DistanceHead for messure the distance of 2 region\n",
    "distance_head = DistanceHead(\n",
    "    infeatures = MM_HIDDEN_SIZE\n",
    ").eval()\n",
    "\n",
    "# LeftRightHead for classify leftright of 2 region\n",
    "leftright_head = LeftRightHead(\n",
    "    infeatures=MM_HIDDEN_SIZE\n",
    ").eval()\n",
    "\n",
    "multiplechoice_head = MultipleChoiceHead(\n",
    "    infeatures=MM_HIDDEN_SIZE\n",
    ").eval()\n",
    "\n",
    "counting_head = CountingHead(\n",
    "    infeatures=MM_HIDDEN_SIZE\n",
    ").eval()\n",
    "print(\"Modules instantiated successfully.\")\n",
    "\n",
    "print(\"\\n--- 3. Simulating Forward Pass Data Flow ---\")\n",
    "print(\"\\n--- Step A: Feature Pooling (Simulating RegionExtractor) ---\")\n",
    "hres_tower_features_rgb, lres_tower_features_rgb = region_extractor.feature_refinement(Fimg_rgb_patches)\n",
    "print(f\"hres_tower_features_rgb (for RGB mask pooling): {hres_tower_features_rgb.shape}\")\n",
    "print(f\"lres_tower_features_rgb (for global projector): {lres_tower_features_rgb.shape}\")\n",
    "print(f\"Fimg_depth_patches (for depth mask pooling): {Fimg_depth_patches.shape}\")\n",
    "\n",
    "\n",
    "unprojected_rgb_regions_list = region_extractor.mask_pooling(hres_tower_features_rgb, masks_list, return_list=True)\n",
    "unprojected_depth_regions_list = region_extractor.mask_pooling(Fimg_depth_patches, masks_list, return_list=True)\n",
    "print(f\"Unprojected RGB region features (sample 0 - 1): {unprojected_rgb_regions_list[0].shape, unprojected_rgb_regions_list[1].shape}\") # Should be [NUM_MASKS_SAMPLE, 1152]\n",
    "print(f\"Unprojected Depth region features (sample 0): {unprojected_depth_regions_list[0].shape, unprojected_depth_regions_list[1].shape}\")# Should be [NUM_MASKS_SAMPLE, 1152]\n",
    "\n",
    "print(\"\\n--- Step B: Region Interaction (RegionFeatureExtractor) ---\")\n",
    "# For batch processing, Need pad and stack these.\n",
    "# Let's test with the first sample for simplicity.\n",
    "unprojected_rgb_regions_s0 = unprojected_rgb_regions_list[0]\n",
    "unprojected_depth_regions_s0 = unprojected_depth_regions_list[0]\n",
    "\n",
    "global_context_features_s0 = lres_tower_features_rgb[0]\n",
    "\n",
    "\n",
    "# module enhances region features using self- and cross-attention\n",
    "enhanced_region_features_s0 = region_feature_extractor_new(\n",
    "    rgb_features=unprojected_rgb_regions_s0,\n",
    "    depth_features=unprojected_depth_regions_s0,\n",
    "    image_features=global_context_features_s0\n",
    ")\n",
    "print(f\"Enhanced region features (sample 0): {enhanced_region_features_s0.shape}\") # Should be [2 * NUM_MASKS_SAMPLE_1, 1152]\n",
    "print(\"\\n--- Step C: Branching for LLM and Auxiliary Heads ---\")\n",
    "\n",
    "print(\"\\n  --- Branch 1 (LLM Pathway) ---\")\n",
    "# Split the enhanced features back into RGB and Depth\n",
    "num_masks_s0 = unprojected_rgb_regions_s0.shape[0]\n",
    "enhanced_rgb_s0 = enhanced_region_features_s0[:num_masks_s0]\n",
    "enhanced_depth_s0 = enhanced_region_features_s0[num_masks_s0:]\n",
    "print(f\"Enhanced RGB region features (sample 0): {enhanced_rgb_s0.shape}\") # Should be [NUM_MASKS_SAMPLE_1, 1152]\n",
    "print(f\"Enhanced Depth region features (sample 0): {enhanced_depth_s0.shape}\") # Should be [NUM_MASKS_SAMPLE_1, 1152]\n",
    "\n",
    "\n",
    "# Pass through the original projectors to get LLM-compatible embeddings\n",
    "projected_rgb_for_llm_s0 = region_extractor.rgb_projector(enhanced_rgb_s0)\n",
    "projected_depth_for_llm_s0 = region_extractor.depth_projector(enhanced_depth_s0)\n",
    "print(f\"Projected RGB for LLM (sample 0): {projected_rgb_for_llm_s0.shape}\") # Should be [num_masks, 4096]\n",
    "print(f\"Projected Depth for LLM (sample 0): {projected_depth_for_llm_s0.shape}\") # Should be [num_masks, 4096]\n",
    "\n",
    "# --- Branch 2: Features for the Auxiliary Task Heads ---\n",
    "print(\"\\n  --- Branch 2 (Auxiliary Heads) ---\")\n",
    "# The task_heads module takes the enhanced features and applies different heads.\n",
    "# Let's test each head individually.\n",
    "\n",
    "# 2.1 - Region Classifier: take all region on image and classify each region\n",
    "region_class_logits = region_head(enhanced_region_features_s0)\n",
    "print(f\"Region Classifier inputs (sample 0): {enhanced_region_features_s0.shape}\") # Should be [num_masks*2, 1152]\n",
    "print(f\"Region Classifier logits (sample 0): {region_class_logits.shape}\") # Should be [num_masks, num_region_classes]\n",
    "\n",
    "# 2.2.1 - Distance Head: take 2 region and predict distance for each region\n",
    "# This head expects features for exactly two regions.\n",
    "# Let's simulate taking the first two regions from the enhanced features.\n",
    "two_region_features = (enhanced_region_features_s0[[0, 1]],enhanced_region_features_s0[[0+num_masks_s0, 1+num_masks_s0]]) # Grabbing RGB and Depth for first 2 masks\n",
    "distance_prediction = distance_head(two_region_features)\n",
    "print(f\"Distance Head inputs (sample 0): tuple of {len(two_region_features)} - for RGB: {two_region_features[0].shape} - for Depth: {two_region_features[1].shape}\") # tuple of (rgb_features, depth_features) each of shape (2, F)\n",
    "print(f\"Distance Head prediction (sample 0): {distance_prediction}\") # Should be sclar (a single distance value)\n",
    "\n",
    "\n",
    "# 2.2.2 - Left/Right Head: take 2 region and predict left/right for each region\n",
    "left_right_logits = leftright_head(two_region_features)\n",
    "print(f\"Left/Right Head inputs (sample 0): tuple of {len(two_region_features)} - for RGB: {two_region_features[0].shape} - for Depth: {two_region_features[1].shape}\") # tuple of (rgb_features, depth_features) each of shape (2, F)\n",
    "print(f\"Left/Right Head logits (sample 0): {left_right_logits}\") # Should be [1, 2] (logits for left/right classes)\n",
    "\n",
    "\n",
    "# 2.2.3 - MultipleChoice Head: This head takes features for all choices and predicts which one is correct.\n",
    "all_region_features = (enhanced_region_features_s0[:num_masks_s0], enhanced_region_features_s0[num_masks_s0:])\n",
    "mcq_logits = multiplechoice_head(all_region_features)\n",
    "print(f\"MultipleChoice Head inputs (sample 0): tuple of {len(all_region_features)} - for RGB: {all_region_features[0].shape} - for Depth: {all_region_features[1].shape}\") # tuple of (rgb_features, depth_features) each of shape (Nr, F)\n",
    "print(f\"MultipleChoice Head logits (sample 0): {mcq_logits}\") # Should be [num_masks]\n",
    "\n",
    "# 2.2.4 - Counting Head: This head fuses all region features and predicts a count.\n",
    "count_logits = counting_head(all_region_features)\n",
    "print(f\"Counting Head inputs (sample 0): tuple of {len(all_region_features)} - for RGB: {all_region_features[0].shape} - for Depth: {all_region_features[1].shape}\") # tuple of (rgb_features, depth_features) each of shape (Nr, F)\n",
    "print(f\"Counting Head logits (sample 0): {count_logits}\")\n",
    "\n",
    "print(\"\\n--- Data Flow Test Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b34e4b6-93cc-4ae1-a0fd-bb8dc7b7bdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1152])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_region_features_s0[[0, 1, 5, 6]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cc7bb60-f63b-4be7-abe3-5d50ad3f9913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1152])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_region_features_s0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05a97c-a269-4de8-af86-48244c01c63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
